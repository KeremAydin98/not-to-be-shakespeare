{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating_Shakespeare.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4TeIpqRqPppJ8X6RqsWIi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeremAydin98/not-to-be-shakespeare/blob/main/Generating_Shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "5GMqd-aNr-T7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h14Y5Y-2lctp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://homl.info/shakespeare\"\n",
        "# File at the origin url is downloaded to the cache dir, final location of the file is placed on the fname in our case it is \"shakespeare.txt\"\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", url)\n",
        "# Open the file with \"with\" command so that we do not need to close it afterwards\n",
        "with open(filepath) as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "vK8_m_1Llkoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b26688-c45d-460b-bc2f-476273ede553"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the first 100 characters of the text\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_OtLA3oqPra",
        "outputId": "4837c0ef-2bed-4ab1-811c-78523477c7c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets use Keras' Tokenizer class"
      ],
      "metadata": {
        "id": "ucktJmeOmisa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the character level tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True) #  char_level: if True, every character will be treated as a token.\n",
        "\n",
        "# Fit it on the text\n",
        "# fit_on_texts: This method creates the vocabulary index based on word frequency. 0 is reserved for padding. So lower integer means more frequent word.\n",
        "tokenizer.fit_on_texts(text)"
      ],
      "metadata": {
        "id": "QtVfkbZ9lxsC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the tokenizer can encode a sentence (or a list of sentences) to a\n",
        "list of character IDs and back, and it tells us how many distinct characters\n",
        "there are and the total number of characters in the text:"
      ],
      "metadata": {
        "id": "IIA-O3nvmfbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, tokenizer is able to transform texts to sequences\n",
        "\n",
        "# \"texts_to_sequences\" basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. \n",
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIky3gCxmTA0",
        "outputId": "90bd6e55-02bc-49d0-f89b-d06b58fbf3f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And sequences to texts\n",
        "tokenizer.sequences_to_texts([[20,6,9,8,3]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhtacxZammeM",
        "outputId": "2957024e-37f9-4511-94b1-3ae0943a9ba5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of distinct characters\n",
        "max_id = len(tokenizer.word_index)\n",
        "\n",
        "# Total number of characters\n",
        "dataset_size = tokenizer.document_count\n",
        "\n",
        "max_id, dataset_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qW4Gz88mrUC",
        "outputId": "9f6b10be-4fcc-4c73-f28b-c96af095597a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39, 1115394)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):"
      ],
      "metadata": {
        "id": "xJs6QQNBnEwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# We subtract 1 to get IDs from 0 to 38, rather than from 1 to 39\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([text])) - 1"
      ],
      "metadata": {
        "id": "iu7An3JFm7j0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1VTrGI12vs3",
        "outputId": "5ea3d983-c5ad-4e23-8aff-539a87a6772f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19,  5,  8, ..., 20, 26, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and validation data split\n",
        "split_size = int(dataset_size * 0.7)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:split_size])"
      ],
      "metadata": {
        "id": "G7INvw4znOfX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set now consists of a single sequence of over a million\n",
        "characters, so we can’t just train the neural network directly on it: the\n",
        "RNN would be equivalent to a deep net with over a million layers, and we\n",
        "would have a single (very long) instance to train it. Instead, we will use\n",
        "the dataset’s window() method to convert this long sequence of characters\n",
        "into many smaller windows of text."
      ],
      "metadata": {
        "id": "bmexZZRPpWX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "\"\"\"\n",
        "Input:\n",
        "[[1,2,3,4,5,6,7,8]]\n",
        "Output:\n",
        "[[1,2,3,4,5],\n",
        "[2,3,4,5,6],\n",
        "[3,4,5,6,7],\n",
        "[4,5,6,7,8]]\n",
        "\"\"\"\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "yTBGo7ZrpAED"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og9TNKkzpkDR",
        "outputId": "43df3b06-1db8-4504-891d-3af656ec8a24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<WindowDataset element_spec=DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([]))>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must call the flat_map()\n",
        "method: it converts a nested dataset into a flat dataset (one that does not\n",
        "contain datasets)"
      ],
      "metadata": {
        "id": "7kw0dGw-rBEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "    map: It returns a new RDD by applying given function to each element of the RDD. Function in map returns only one item.\n",
        "\n",
        "    flatMap: Similar to map, it returns a new RDD by applying a function to each element of the RDD, but output is flattened.\n",
        "\n",
        "\"\"\"\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ],
      "metadata": {
        "id": "EsYLEWTTp436"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "# We seperate the data into 8 batches and then shuffle it, in the end drop the remained data\n",
        "dataset = dataset.shuffle(10000).batch(batch_size,drop_remainder=True)\n",
        "\n",
        "# At this one we seperate target and input from the dataset\n",
        "\"\"\"\n",
        "Input:\n",
        "[[1,2,3,4,5],\n",
        "[2,3,4,5,6],\n",
        "[3,4,5,6,7],\n",
        "[4,5,6,7,8]]\n",
        "Output:\n",
        "\n",
        "Input: [[1,2,3,4], Target: [2,3,4,5]]\n",
        "       [[2,3,4,5], [3,4,5,6]]\n",
        "\"\"\"\n",
        "dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:,1:]))"
      ],
      "metadata": {
        "id": "kE0vwOloq7m4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we do a one hot encoding on the input data so that loss function would make sense\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
      ],
      "metadata": {
        "id": "g7UcuxitraDF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "cNR25pXDrxT4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Model"
      ],
      "metadata": {
        "id": "eDzZqHMNr8tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.GRU(512, return_sequences=True, \n",
        "                             input_shape = [None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
        "                             tf.keras.layers.GRU(512, return_sequences=True,\n",
        "                             dropout=0.2, recurrent_dropout=0.2),\n",
        "                             tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_id, activation=\"softmax\")) # This wrapper allows to apply a layer to every temporal slice of an input.\n",
        "])\n",
        "\n",
        "\"\"\"\n",
        "TimeDistributed:\n",
        "\n",
        "  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with channels_last data format, across 10 timesteps. The batch input shape is (32, 10, 128, 128, 3).\n",
        "\n",
        "  You can then use TimeDistributed to apply the same Conv2D layer to each of the 10 timesteps, independently\n",
        "\n",
        "  Because TimeDistributed applies the same instance of Conv2D to each of the timestamps, the same set of weights are used at each timestamp.\n",
        "\"\"\"\n",
        "\n",
        "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, # even though input is one hot encoded, target is still tokenized, so we must use sparse categorical cross entropy\n",
        "              optimizer=tf.keras.optimizers.Adam())"
      ],
      "metadata": {
        "id": "Dfx2YPp1sBmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f22abc-1a0e-4f14-fcd0-7a7f3ae018cf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V__CagHGs_I1",
        "outputId": "a591ba40-b9d2-43d7-b371-4b41892d507f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, None, 512)         849408    \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, None, 512)         1575936   \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, None, 39)         20007     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,445,351\n",
            "Trainable params: 2,445,351\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the model"
      ],
      "metadata": {
        "id": "DkvzbtsYUVfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset,steps_per_epoch=500, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GvvAX57syno",
        "outputId": "5cffcec5-2ce7-4a64-da26-4c08beec10b9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 248s 476ms/step - loss: 2.0979\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 260s 520ms/step - loss: 1.3120\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 242s 483ms/step - loss: 1.0117\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 263s 527ms/step - loss: 0.9270\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 282s 564ms/step - loss: 0.8933\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 266s 532ms/step - loss: 0.8847\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 241s 482ms/step - loss: 0.8788\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 238s 477ms/step - loss: 0.8670\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 235s 471ms/step - loss: 0.8533\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 238s 475ms/step - loss: 0.8250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a Shakespeare text"
      ],
      "metadata": {
        "id": "k2rZdf81UZMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(texts):\n",
        "  # Preprocessing the text by first tokenizing and then one hot encoding the input\n",
        "  x = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "  return tf.one_hot(x, max_id)"
      ],
      "metadata": {
        "id": "VdpyoivAs2dd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "  X_new = preprocess([text])\n",
        "  y_probs = model.predict(X_new)[0,-1:,:]\n",
        "  rescaled_logits = tf.math.log(y_probs) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "metadata": {
        "id": "7-uFys7dMYRM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_text(text, n_chars=1000, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "metadata": {
        "id": "NMSd-OzpNC38"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"r\",temperature=1))"
      ],
      "metadata": {
        "id": "2f2EBusuNQWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0abc96-5102-4176-e7a3-6a0c34496d25"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rticus: i, without note,-here's\n",
            "a vertel tears with smiljnech.\n",
            "\n",
            "second officer:\n",
            "faith, there had been many, or elumy to reward\n",
            "whihe he remember'd.\n",
            "a very on your actions and daugk,\n",
            "that may fully tubly care edsured here's anly arm detter and the bleared sightry\n",
            "sevond the common people.\n",
            "\n",
            "second officer:\n",
            "has he did budgen deeds doull\n",
            "\n",
            "brutus:\n",
            "i will give them make i as liqy as little question\n",
            "as he is proud to do't.\n",
            "\n",
            "brutus:\n",
            "what's the mad me clip than a never o hate\n",
            "he will not bloody bleading:\n",
            "if he did so did at the common disposition.\n",
            "\n",
            "sicinius:\n",
            "he cannot temperately that may fully discover his\n",
            "the arm our stand, as bard as he hath\n",
            "displeasure your sulvessers: set him speak: matrons flung gloves,\n",
            "let country? he was he wounded?\n",
            "god sand carry with us;\n",
            "for sinking under thee; you are knowen part of your ay, such a nettle but they\n",
            "plasing beee: they love or hate\n",
            "him men true.\n",
            "where is he wounded?\n",
            "god save you give me to care whether\n",
            "the people is tho market-place nor on him our\n",
            "putter\n"
          ]
        }
      ]
    }
  ]
}